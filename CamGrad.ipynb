{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim \n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5150b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifica questo con il percorso alla tua cartella\n",
    "data_dir = r'binary_one_type_split'\n",
    "\n",
    "# Dimensione standard, modificabile\n",
    "image_size = 256\n",
    "\n",
    "# Trasformazioni da applicare: resize, tensorizzazione e normalizzazione\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),  # converte in [0,1]\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # per 3 canali\n",
    "])\n",
    "\n",
    "# Dataset per train e validation\n",
    "train_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=transform)\n",
    "\n",
    "# DataLoader per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b04bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),    # (32, 128, 128)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),   # (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # (128, 32, 32)\n",
    "            nn.BatchNorm2d(128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*32*32, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128*32*32),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 32, 32)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # (64, 64, 64)\n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # (32, 128, 128)\n",
    "            nn.BatchNorm2d(32),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),    # (3, 256, 256)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorClassifier(nn.Module):\n",
    "    def __init__(self, encoder, latent_dim=128):\n",
    "        super().__init__()\n",
    "        # Congela l'encoder, tranne ultimo layer\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for i in range(6, len(encoder)): # Unfreeze from index 6 till the end of encoder\n",
    "            for param in encoder[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.encoder = encoder  # encoder giÃ  addestrato\n",
    "\n",
    "        # Head di classificazione\n",
    "        self.classifier = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 128),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    \n",
    "                    nn.Linear(128, 64),\n",
    "                    nn.BatchNorm1d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    \n",
    "                    nn.Linear(64, 1)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # ottieni le feature compresse\n",
    "        return self.classifier(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccf1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_14680\\2890562879.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"autoencoder.pth\"))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Autoencoder(latent_dim=128)\n",
    "model.load_state_dict(torch.load(\"autoencoder.pth\"))\n",
    "model.eval()  \n",
    "\n",
    "encoder = model.encoder\n",
    "\n",
    "# Crea il classificatore completo\n",
    "classifier_model = TumorClassifier(encoder, latent_dim=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d45961",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Usa BCEWithLogitsLoss, quindi togliamo la sigmoid nella classifier (opzionale ma consigliato)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier_model.parameters()), lr=1e-4)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)  # shape (B, 1)\n",
    "\n",
    "        outputs = classifier_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ----------------------\n",
    "    # ðŸ§ª Valutazione su test set\n",
    "    classifier_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = classifier_model(images)\n",
    "            preds = torch.sigmoid(outputs) > 0.75  # binarizza le predizioni\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calcolo metriche\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41613c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        self.target_layer.register_forward_hook(self._save_activation)\n",
    "        self.target_layer.register_full_backward_hook(self._save_gradient)\n",
    "\n",
    "    def _save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def __call__(self, input_image, target_class=None):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            # If target_class is not specified, take the class with the highest probability\n",
    "            target_class = torch.sigmoid(output).argmax().item()\n",
    "\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        one_hot_output = torch.zeros_like(output)\n",
    "        one_hot_output[:, target_class] = 1\n",
    "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "        # Get gradients and activations\n",
    "        guided_gradients = self.gradients\n",
    "        activations = self.activations\n",
    "\n",
    "        # Compute weights (global average pooling of gradients)\n",
    "        weights = torch.mean(guided_gradients, dim=[2, 3], keepdim=True)\n",
    "\n",
    "        # Compute CAM\n",
    "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
    "        cam = F.relu(cam) # Apply ReLU to remove negative values\n",
    "\n",
    "        # Normalize CAM to 0-1\n",
    "        cam = F.interpolate(cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max()\n",
    "\n",
    "        return cam.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bc1ea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (NoneType, dim=list, keepdim=bool), but expected one of:\n * (Tensor input, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m grad_cam \u001b[38;5;241m=\u001b[39m GradCAM(classifier_model, classifier_model\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;241m6\u001b[39m])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Get CAM\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m cam_output \u001b[38;5;241m=\u001b[39m grad_cam(sample_image)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# De-normalize and display the original image\u001b[39;00m\n\u001b[0;32m     34\u001b[0m original_image_display \u001b[38;5;241m=\u001b[39m (sample_image\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 40\u001b[0m, in \u001b[0;36mGradCAM.__call__\u001b[1;34m(self, input_image, target_class)\u001b[0m\n\u001b[0;32m     37\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Compute weights (global average pooling of gradients)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(guided_gradients, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Compute CAM\u001b[39;00m\n\u001b[0;32m     43\u001b[0m cam \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(weights \u001b[38;5;241m*\u001b[39m activations, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (NoneType, dim=list, keepdim=bool), but expected one of:\n * (Tensor input, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n * (Tensor input, tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "# --- Usage Example for Grad-CAM ---\n",
    "\n",
    "# Load a sample image from the test set\n",
    "model.eval() # Ensure the autoencoder's encoder is in eval mode\n",
    "classifier_model.eval() # Ensure the classifier is in eval mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(test_loader))\n",
    "    sample_image = images[0].unsqueeze(0).to(device) # Take the first image\n",
    "    true_label = labels[0].item()\n",
    "\n",
    "# Instantiate GradCAM\n",
    "# We need to target the last convolutional layer of the encoder within the classifier model\n",
    "# In your Autoencoder, the encoder's last Conv2d layer is at index 6:\n",
    "# nn.Sequential(\n",
    "#     nn.Conv2d(3, 32, ...), # 0\n",
    "#     nn.BatchNorm2d(32),   # 1\n",
    "#     nn.ReLU(),            # 2\n",
    "#     nn.Conv2d(32, 64, ...), # 3\n",
    "#     nn.BatchNorm2d(64),   # 4\n",
    "#     nn.ReLU(),            # 5\n",
    "#     nn.Conv2d(64, 128, ...), # 6 <--- Target layer for Grad-CAM\n",
    "#     nn.BatchNorm2d(128),  # 7\n",
    "#     nn.ReLU(),            # 8\n",
    "#     nn.Flatten(),         # 9\n",
    "#     nn.Linear(128*32*32, latent_dim) # 10\n",
    "# )\n",
    "grad_cam = GradCAM(classifier_model, classifier_model.encoder[6])\n",
    "\n",
    "# Get CAM\n",
    "cam_output = grad_cam(sample_image)\n",
    "\n",
    "# De-normalize and display the original image\n",
    "original_image_display = (sample_image.squeeze().cpu().permute(1, 2, 0).numpy() * 0.5) + 0.5\n",
    "\n",
    "# Overlay CAM on the original image\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam_output), cv2.COLORMAP_JET)\n",
    "heatmap = np.float32(heatmap) / 255\n",
    "heatmap = heatmap[..., ::-1] # Convert BGR to RGB\n",
    "\n",
    "# Superimpose the heatmap on the original image\n",
    "# Ensure the image is in the range [0, 1] before superposition\n",
    "superimposed_img = original_image_display + heatmap\n",
    "superimposed_img = superimposed_img / np.max(superimposed_img) # Normalize to prevent clipping\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original_image_display)\n",
    "plt.title(f\"Original Image (True Label: {true_label})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(heatmap)\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(superimposed_img)\n",
    "predicted_logit = classifier_model(sample_image).item()\n",
    "predicted_prob = torch.sigmoid(torch.tensor(predicted_logit)).item()\n",
    "predicted_class = 1 if predicted_prob > 0.5 else 0 # Assuming 0.5 threshold for prediction\n",
    "plt.title(f\"Superimposed (Pred: {predicted_class}, Prob: {predicted_prob:.2f})\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
